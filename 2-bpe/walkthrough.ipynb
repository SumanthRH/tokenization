{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple walkthrough for training your very own BPE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load your training corpus\n",
    "\n",
    "We'll be using a dummy training corpus with 4 lines of text in `ex_corpus.txt`. Here's how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to learn about BPE\n",
      "I'm learning about byte-pair encoding\n",
      "My friend learnt that digram coding and byte pair encoding mean the same\n",
      "I love Jacques Cousteau\n"
     ]
    }
   ],
   "source": [
    "with open(\"ex_corpus.txt\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract a list of words from the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "EOW_TOKEN = '</w>'\n",
    "def get_initial_words(filename):\n",
    "    segmented_word_to_freq = defaultdict(int) # {\"w o r d </w>\": 1, ...}\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split() # words are split on whitespace; this is our pre-tokenization step\n",
    "            for word in words:\n",
    "                # Separate each character with spaces, to show tokens for clarity. \"word\" -> \"w o r d </w>\"\n",
    "                segmented_word = ' '.join(list(word)) + \" \" +  EOW_TOKEN\n",
    "                segmented_word_to_freq[segmented_word] += 1\n",
    "    return segmented_word_to_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'T r y i n g </w>': 1, 't o </w>': 1, 'l e a r n </w>': 1, 'a b o u t </w>': 2, 'B P E </w>': 1, \"I ' m </w>\": 1, 'l e a r n i n g </w>': 1, 'b y t e - p a i r </w>': 1, 'e n c o d i n g </w>': 2, 'M y </w>': 1, 'f r i e n d </w>': 1, 'l e a r n t </w>': 1, 't h a t </w>': 1, 'd i g r a m </w>': 1, 'c o d i n g </w>': 1, 'a n d </w>': 1, 'b y t e </w>': 1, 'p a i r </w>': 1, 'm e a n </w>': 1, 't h e </w>': 1, 's a m e </w>': 1, 'I </w>': 1, 'l o v e </w>': 1, 'J a c q u e s </w>': 1, 'C o u s t e a u </w>': 1})\n"
     ]
    }
   ],
   "source": [
    "word_to_freq = get_initial_words('ex_corpus.txt')\n",
    "print(word_to_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the words are actually represented as a space-separated string of symbols/tokens (i.e the dictionary has _segmented_ words)- in this case, we start off with simple character-level tokenization, and add an end of word token as well to match the original BPE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get initial vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'T': 0, 'r': 1, 'y': 2, 'i': 3, 'n': 4, 'g': 5, '</w>': 6, 't': 7, 'o': 8, 'l': 9, 'e': 10, 'a': 11, 'b': 12, 'u': 13, 'B': 14, 'P': 15, 'E': 16, 'I': 17, \"'\": 18, 'm': 19, '-': 20, 'p': 21, 'c': 22, 'd': 23, 'M': 24, 'f': 25, 'h': 26, 's': 27, 'v': 28, 'J': 29, 'q': 30, 'C': 31})\n"
     ]
    }
   ],
   "source": [
    "def get_tokens(word_to_freq):\n",
    "    tokens = defaultdict(int)\n",
    "    for word in word_to_freq.keys():\n",
    "        for token in word.split():\n",
    "            if token not in tokens:\n",
    "                tokens[token] = len(tokens)\n",
    "    return tokens\n",
    "\n",
    "vocab = get_tokens(word_to_freq)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vocabulary is a mapping from token to a unique token ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisit the training algorithm\n",
    "\n",
    "1. Extract a list of words from the training corpus. (**Done**)\n",
    "2. Make a word counter dictionary with keys being words and values being frequencies in the training corpus. (**Done**)\n",
    "3. Keep a vocabulary of symbols (variable length strings), initialized with unique characters present in the training corpus. (**Done**)\n",
    "\n",
    "We now proceed to the core part of the algorithm:\n",
    "\n",
    "4. Initialize a list of _merge rules_ to be an empty. Each merge rule is a tuple of symbol/token to merge at test time.\n",
    "4. Iteratively do the following:\n",
    "    - Get the most frequent pair of symbols in the current vocabulary, by going over the word counter. (Ex: `(\"l\", \"e\")`)\n",
    "    - Merge the two symbols into a new symbol, and _add_ this to the vocabulary. This is like a new byte in the original BPE except we have variable length strings (_character n-gram_). \n",
    "    - Add our tuple of symbols to the list of merge rules.\n",
    "    - Replace all occurences of the pair of symbols with the new symbol. For example, a word, segmented as `(\"l\", \"e\", \"a\", \"r\", \"n\")` becomes `(\"le\", \"a\", \"r\", \"n\")`. \n",
    "    - Repeat until you reach the target vocabulary size. (a hyperparameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training algorithm should look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merges = []\n",
    "# num_merges = N\n",
    "# for i in range(num_merges):\n",
    "#     pairs = <get statistics for frequencies of symbol pairs>\n",
    "#     best_pair = max(pairs, key=pairs.get) -----> Do argmax for pairs, get the most frequent pair\n",
    "#     word_to_freq = merge_word_splits(best_pair, word_to_freq) -----> Merge the most frequent pair and get the new words\n",
    "#     new_token = ''.join(best_pair) -----> Get the new token\n",
    "#     vocab[new_token] = len(vocab) -----> Add the new token to the vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full algorithm is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocab:  defaultdict(<class 'int'>, {'T': 0, 'r': 1, 'y': 2, 'i': 3, 'n': 4, 'g': 5, '</w>': 6, 't': 7, 'o': 8, 'l': 9, 'e': 10, 'a': 11, 'b': 12, 'u': 13, 'B': 14, 'P': 15, 'E': 16, 'I': 17, \"'\": 18, 'm': 19, '-': 20, 'p': 21, 'c': 22, 'd': 23, 'M': 24, 'f': 25, 'h': 26, 's': 27, 'v': 28, 'J': 29, 'q': 30, 'C': 31})\n",
      "##################\n",
      "Iteration 1\n",
      "Best pair:  ('i', 'n')\n",
      "New token:  in\n",
      "All words:  ['T r y in g </w>', 't o </w>', 'l e a r n </w>', 'a b o u t </w>', 'B P E </w>', \"I ' m </w>\", 'l e a r n in g </w>', 'b y t e - p a i r </w>', 'e n c o d in g </w>', 'M y </w>', 'f r i e n d </w>', 'l e a r n t </w>', 't h a t </w>', 'd i g r a m </w>', 'c o d in g </w>', 'a n d </w>', 'b y t e </w>', 'p a i r </w>', 'm e a n </w>', 't h e </w>', 's a m e </w>', 'I </w>', 'l o v e </w>', 'J a c q u e s </w>', 'C o u s t e a u </w>']\n",
      "##################\n",
      "Iteration 2\n",
      "Best pair:  ('in', 'g')\n",
      "New token:  ing\n",
      "All words:  ['T r y ing </w>', 't o </w>', 'l e a r n </w>', 'a b o u t </w>', 'B P E </w>', \"I ' m </w>\", 'l e a r n ing </w>', 'b y t e - p a i r </w>', 'e n c o d ing </w>', 'M y </w>', 'f r i e n d </w>', 'l e a r n t </w>', 't h a t </w>', 'd i g r a m </w>', 'c o d ing </w>', 'a n d </w>', 'b y t e </w>', 'p a i r </w>', 'm e a n </w>', 't h e </w>', 's a m e </w>', 'I </w>', 'l o v e </w>', 'J a c q u e s </w>', 'C o u s t e a u </w>']\n",
      "##################\n",
      "Iteration 3\n",
      "Best pair:  ('ing', '</w>')\n",
      "New token:  ing</w>\n",
      "All words:  ['T r y ing</w>', 't o </w>', 'l e a r n </w>', 'a b o u t </w>', 'B P E </w>', \"I ' m </w>\", 'l e a r n ing</w>', 'b y t e - p a i r </w>', 'e n c o d ing</w>', 'M y </w>', 'f r i e n d </w>', 'l e a r n t </w>', 't h a t </w>', 'd i g r a m </w>', 'c o d ing</w>', 'a n d </w>', 'b y t e </w>', 'p a i r </w>', 'm e a n </w>', 't h e </w>', 's a m e </w>', 'I </w>', 'l o v e </w>', 'J a c q u e s </w>', 'C o u s t e a u </w>']\n",
      "##################\n",
      "Iteration 4\n",
      "Best pair:  ('e', 'a')\n",
      "New token:  ea\n",
      "All words:  ['T r y ing</w>', 't o </w>', 'l ea r n </w>', 'a b o u t </w>', 'B P E </w>', \"I ' m </w>\", 'l ea r n ing</w>', 'b y t e - p a i r </w>', 'e n c o d ing</w>', 'M y </w>', 'f r i e n d </w>', 'l ea r n t </w>', 't h a t </w>', 'd i g r a m </w>', 'c o d ing</w>', 'a n d </w>', 'b y t e </w>', 'p a i r </w>', 'm ea n </w>', 't h e </w>', 's a m e </w>', 'I </w>', 'l o v e </w>', 'J a c q u e s </w>', 'C o u s t ea u </w>']\n",
      "##################\n",
      "Iteration 5\n",
      "Best pair:  ('t', '</w>')\n",
      "New token:  t</w>\n",
      "All words:  ['T r y ing</w>', 't o </w>', 'l ea r n </w>', 'a b o u t</w>', 'B P E </w>', \"I ' m </w>\", 'l ea r n ing</w>', 'b y t e - p a i r </w>', 'e n c o d ing</w>', 'M y </w>', 'f r i e n d </w>', 'l ea r n t</w>', 't h a t</w>', 'd i g r a m </w>', 'c o d ing</w>', 'a n d </w>', 'b y t e </w>', 'p a i r </w>', 'm ea n </w>', 't h e </w>', 's a m e </w>', 'I </w>', 'l o v e </w>', 'J a c q u e s </w>', 'C o u s t ea u </w>']\n",
      "##################\n",
      "Iteration 6 done\n",
      "Iteration 7 done\n",
      "Iteration 8 done\n",
      "Iteration 9 done\n",
      "Iteration 10 done\n",
      "Iteration 11 done\n",
      "Iteration 12 done\n",
      "Iteration 13 done\n",
      "Iteration 14 done\n",
      "Iteration 15 done\n",
      "Final vocab:  defaultdict(<class 'int'>, {'T': 0, 'r': 1, 'y': 2, 'i': 3, 'n': 4, 'g': 5, '</w>': 6, 't': 7, 'o': 8, 'l': 9, 'e': 10, 'a': 11, 'b': 12, 'u': 13, 'B': 14, 'P': 15, 'E': 16, 'I': 17, \"'\": 18, 'm': 19, '-': 20, 'p': 21, 'c': 22, 'd': 23, 'M': 24, 'f': 25, 'h': 26, 's': 27, 'v': 28, 'J': 29, 'q': 30, 'C': 31, 'in': 32, 'ing': 33, 'ing</w>': 34, 'ea': 35, 't</w>': 36, 'e</w>': 37, 'lea': 38, 'lear': 39, 'learn': 40, 'ou': 41, 'en': 42, 'co': 43, 'cod': 44, 'coding</w>': 45, 'ab': 46})\n",
      "All merges:  [('i', 'n'), ('in', 'g'), ('ing', '</w>'), ('e', 'a'), ('t', '</w>'), ('e', '</w>'), ('l', 'ea'), ('lea', 'r'), ('lear', 'n'), ('o', 'u'), ('e', 'n'), ('c', 'o'), ('co', 'd'), ('cod', 'ing</w>'), ('a', 'b')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def get_stats(word_to_freq: dict):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in word_to_freq.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq \n",
    "    return pairs\n",
    "\n",
    "def merge_word_splits(pair, v_in: dict):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "merges = []\n",
    "print(\"Initial vocab: \", vocab)\n",
    "print(\"##################\")\n",
    "num_merges = 15\n",
    "print_until = 5\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(word_to_freq)\n",
    "    best_pair = max(pairs, key=pairs.get)\n",
    "    word_to_freq = merge_word_splits(best_pair, word_to_freq)\n",
    "    new_token = ''.join(best_pair)\n",
    "    vocab[new_token] = len(vocab)\n",
    "    merges.append(best_pair)\n",
    "    if i < print_until:\n",
    "        print(f\"Iteration {i+1}\")\n",
    "        print(\"Best pair: \", best_pair)\n",
    "        print(\"New token: \", new_token)\n",
    "        print(\"All words: \", list(word_to_freq.keys()))\n",
    "        print(\"##################\")\n",
    "    else:\n",
    "        print(f\"Iteration {i+1} done\")\n",
    "print(\"Final vocab: \", vocab)\n",
    "print(\"All merges: \", merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I've only printed out the full training state until 5 iterations. If you want to see all of it, change `print_until` to be `num_merges`. Before we analyse what happened, let's see how our initial set of words get tokenized with the final dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T r y ing</w>', 't o </w>', 'learn </w>', 'ab ou t</w>', 'B P E </w>', \"I ' m </w>\", 'learn ing</w>', 'b y t e - p a i r </w>', 'en coding</w>', 'M y </w>', 'f r i en d </w>', 'learn t</w>', 't h a t</w>', 'd i g r a m </w>', 'coding</w>', 'a n d </w>', 'b y t e</w>', 'p a i r </w>', 'm ea n </w>', 't h e</w>', 's a m e</w>', 'I </w>', 'l o v e</w>', 'J a c q u e s </w>', 'C ou s t ea u </w>']\n"
     ]
    }
   ],
   "source": [
    "print(list(word_to_freq.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe what happens to the last line in the text: This is an out-of-place sentence compared to the above three. You're getting almost character-level tokenization for all the words here(\"love\" gets 4 tokens), while words that are repeated (\"learn\", \"encoding\") get represented with 1/2 tokens. This is the essence of BPE: _outliers_ do _not_ get _compressed_ as much. When you have a large training corpus, it's very rare to get \"outlier\" English sentences, but you can imagine other data domains (code for example) that wasn't represented well in the training corpus, to be segmentd into a lot of tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

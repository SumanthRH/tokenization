{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple walkthrough for a minimal HuggingFace Tokenizer implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the jupyter notebook fans out there, this is a scrappy walkthrough for all the different parts in the implementation for the `BPE` and the `MySlowTokenizer` classes implemented here. If you haven't gone through the chapter README yet, please do! The gist is that HuggingFace's [PreTokenizer class](https://huggingface.co/docs/transformers/main_classes/tokenizer) implements a \"slow\" tokenizer in python. `PreTokenizer` inherits from `PreTokenizerBase`, and a `SpecialTokensMixin` class. Thus, if you're trying to understand the slow tokenizer for GPT-2, you have a whole class genealogy to figure out what is implemented where:\n",
    "```\n",
    "PreTokenizerBase       SpecialTokensMixin\n",
    "        \\                   /\n",
    "         \\                 /\n",
    "          PreTrainedTokenizer\n",
    "                  |\n",
    "                  |\n",
    "            GPT2Tokenizer\n",
    "```\n",
    "This is not pretty to navigate. So, I've tried to make things simple with just 2 classes:\n",
    "\n",
    "1. `BPE` : Meant to replicate what the BPE algorithm does in GPT-2's tokenizer. This is not a complete tokenizer in itself, it's got some weird kinks (with unicode symbols, etc) that we'll see soon.\n",
    "2. `MySlowTokenizer` : This is meant to a minimal implementation that can match all the basic features for HuggingFace's GPT2 tokenizer (the slow version).\n",
    "\n",
    "We're going to completely ignore the special tokens handling for now, as this is related to postprocessing that is in fact easier to understand later. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE\n",
    "Let's see our implementation in action first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sumanthrh/anaconda3/envs/huggingface/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bpe import BPE \n",
    "bpe = BPE(\"vocab.json\") # make sure you're in the current directory of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `__call__` method\n",
    "Let's see what the `__call__` method does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bpe.py:56: UserWarning: Word contains whitespaces. Encoding to unicode strings...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ġword aaa'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe(\" wordaaa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output for \" wordaaa\" from the BPE tokenizer is the strange string \"Ġword aaa\". Well, here's what it does:\n",
    "1. Converts input string into bytes and encodes each byte with a unicode symbol. Specifically, a space \" \" becomes \"Ġ\". This is how GPT-2 does it. \n",
    "2. Tokenizes the string into characters \n",
    "3. Applies the BPE merge algorithm to iteratively combine intermediate tokens until you can't reduce it further.\n",
    "4. Join the list of tokens with a whitespace and return the resultant string\n",
    "\n",
    "Before we dive into the implementation for the merge algorithm, let's see what the list of tokens are when you use GPT2's tokenizer from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF's tokens:  [' word', 'aaa']\n",
      "My tokens:  ['Ġword', 'aaa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bpe.py:56: UserWarning: Word contains whitespaces. Encoding to unicode strings...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=False)\n",
    "text = \" wordaaa\"\n",
    "hf_tokens = gpt2_tokenizer.batch_decode(gpt2_tokenizer.encode(text))\n",
    "my_tokens = bpe(text).split(\" \")\n",
    "print(\"HF's tokens: \", hf_tokens)\n",
    "print(\"My tokens: \", my_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two outputs should infact be exactly the same, except for special characters (whitespace, etc) which we'll not get into later. (these two classes aren't exactly comparable as I mentioned, but it's good to see that this work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "Here's the complete merge algorithm with bpe, showing as a standalone function (with unicode symbols decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpe import get_pairs\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "byte_encoder = bytes_to_unicode()\n",
    "byte_decoder = {v: k for k, v in byte_encoder.items()}\n",
    "bpe_ranks = bpe.bpe_ranks\n",
    "\n",
    "def split_into_tokens(word: str):\n",
    "    word = \"\".join([byte_encoder[b] for b in word.encode(\"utf-8\")])\n",
    "    pairs = get_pairs(word) # \"obobc\" -> set([(\"o\", \"b\"), (\"b\", \"o\"), (\"b\", \"c\")])\n",
    "    iter = 0\n",
    "    while True:\n",
    "        # get pair of chars/tokens with lowest rank and merge\n",
    "        bigram = min(pairs, key=lambda pair: bpe_ranks.get(pair, float(\"inf\")))\n",
    "        if bigram not in bpe_ranks:\n",
    "            break # no more mergeable pairs\n",
    "        first, second = bigram\n",
    "        print(f\"{iter=} : {first=} {second=}\")\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i) # find index of occurence of `first` token in word[i:]\n",
    "            except ValueError:\n",
    "                print(f\"\\t -> {iter=} : {first=} not found in {word[i:]=}\")\n",
    "                new_word.extend(word[i:]) \n",
    "                break\n",
    "            else:\n",
    "                print(f\"\\t -> {iter=} : Found {first=} in {word[i:]=}. Skipping previous tokens {word[i:j]}\")\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "\n",
    "            if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
    "                print(f\"\\t -> {iter=} : Merging {first=} and {second=}\")\n",
    "                new_word.append(first + second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "            iter += 1\n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        if len(word) == 1: # merged into a single token\n",
    "            break\n",
    "        else:\n",
    "            pairs = get_pairs(word)\n",
    "        print(f\"{iter=} : Updated {word=}, {pairs=}\")\n",
    "    word = \" \".join(word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=0 : first='Ġ' second='w'\n",
      "\t -> iter=0 : Found first='Ġ' in word[i:]='Ġwordaaa'. Skipping previous tokens \n",
      "\t -> iter=0 : Merging first='Ġ' and second='w'\n",
      "\t -> iter=1 : first='Ġ' not found in word[i:]='ordaaa'\n",
      "iter=1 : Updated word=('Ġw', 'o', 'r', 'd', 'a', 'a', 'a'), pairs={('d', 'a'), ('a', 'a'), ('r', 'd'), ('Ġw', 'o'), ('o', 'r')}\n",
      "iter=1 : first='o' second='r'\n",
      "\t -> iter=1 : Found first='o' in word[i:]=('Ġw', 'o', 'r', 'd', 'a', 'a', 'a'). Skipping previous tokens ('Ġw',)\n",
      "\t -> iter=1 : Merging first='o' and second='r'\n",
      "\t -> iter=2 : first='o' not found in word[i:]=('d', 'a', 'a', 'a')\n",
      "iter=2 : Updated word=('Ġw', 'or', 'd', 'a', 'a', 'a'), pairs={('Ġw', 'or'), ('or', 'd'), ('d', 'a'), ('a', 'a')}\n",
      "iter=2 : first='Ġw' second='or'\n",
      "\t -> iter=2 : Found first='Ġw' in word[i:]=('Ġw', 'or', 'd', 'a', 'a', 'a'). Skipping previous tokens ()\n",
      "\t -> iter=2 : Merging first='Ġw' and second='or'\n",
      "\t -> iter=3 : first='Ġw' not found in word[i:]=('d', 'a', 'a', 'a')\n",
      "iter=3 : Updated word=('Ġwor', 'd', 'a', 'a', 'a'), pairs={('d', 'a'), ('Ġwor', 'd'), ('a', 'a')}\n",
      "iter=3 : first='Ġwor' second='d'\n",
      "\t -> iter=3 : Found first='Ġwor' in word[i:]=('Ġwor', 'd', 'a', 'a', 'a'). Skipping previous tokens ()\n",
      "\t -> iter=3 : Merging first='Ġwor' and second='d'\n",
      "\t -> iter=4 : first='Ġwor' not found in word[i:]=('a', 'a', 'a')\n",
      "iter=4 : Updated word=('Ġword', 'a', 'a', 'a'), pairs={('Ġword', 'a'), ('a', 'a')}\n",
      "iter=4 : first='a' second='a'\n",
      "\t -> iter=4 : Found first='a' in word[i:]=('Ġword', 'a', 'a', 'a'). Skipping previous tokens ('Ġword',)\n",
      "\t -> iter=4 : Merging first='a' and second='a'\n",
      "\t -> iter=5 : Found first='a' in word[i:]=('a',). Skipping previous tokens ()\n",
      "iter=6 : Updated word=('Ġword', 'aa', 'a'), pairs={('aa', 'a'), ('Ġword', 'aa')}\n",
      "iter=6 : first='aa' second='a'\n",
      "\t -> iter=6 : Found first='aa' in word[i:]=('Ġword', 'aa', 'a'). Skipping previous tokens ('Ġword',)\n",
      "\t -> iter=6 : Merging first='aa' and second='a'\n",
      "iter=7 : Updated word=('Ġword', 'aaa'), pairs={('Ġword', 'aaa')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ġword aaa'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_into_tokens(\" wordaaa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how with each iteration, BPE merges the pair of tokens with the lowest rank / highest priority. It has to merge all occurences of the pair, and thus you have two while loops. The brief summary of what happened:\n",
    "1. Obtain all unique bigrams (pairs of adjacent symbols) from the word. (before the while loop)\n",
    "2. State: `word` is initially a string, but it is converted into a tuple in later iterations. `word` represents the current segmentation of the original word, represented as a tuple of tokens.\n",
    "3. The algorithm then enters a loop where it looks for the lowest-ranked bigram (the first merge that BPE learnt while training). This represents the most frequent pair to be merged, or rather, the best compression step that BPE knows for this word.\n",
    "4. If the bigram is not in `bpe_ranks`, it means that no more merges are possible, and the process terminates.\n",
    "    - Why? Observe how the minimum has been calculated. If the bigram with minimum rank is not in `bpe_ranks`, then it has a rank of `inf`, which means none of the other bigrams are in `bpe_ranks` / merge list.\n",
    "5. Otherwise, the function reconstructs the word by merging instances of the bigram.\n",
    "    - This is the inner while loop. We iteratively find the first occurence of `first` in `word`, record the index, and break up the word into two at this index. Keep proceeding till the end of `word`.\n",
    "6. This process continues until the word cannot be further merged, at which point the final tokenized word is returned by joining all the tokens with a whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we flipped the order of the tokens? How do the merges look then? Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=0 : first='Ġ' second='w'\n",
      "\t -> iter=0 : Found first='Ġ' in word[i:]='aaaĠword'. Adding new token aaa\n",
      "\t -> iter=1 : first='Ġ' not found in word[i:]='ord'\n",
      "iter=1 : Updated word=('a', 'a', 'a', 'Ġw', 'o', 'r', 'd'), pairs={('a', 'a'), ('r', 'd'), ('Ġw', 'o'), ('o', 'r'), ('a', 'Ġw')}\n",
      "iter=1 : first='o' second='r'\n",
      "\t -> iter=1 : Found first='o' in word[i:]=('a', 'a', 'a', 'Ġw', 'o', 'r', 'd'). Adding new token ('a', 'a', 'a', 'Ġw')\n",
      "\t -> iter=2 : first='o' not found in word[i:]=('d',)\n",
      "iter=2 : Updated word=('a', 'a', 'a', 'Ġw', 'or', 'd'), pairs={('Ġw', 'or'), ('or', 'd'), ('a', 'a'), ('a', 'Ġw')}\n",
      "iter=2 : first='Ġw' second='or'\n",
      "\t -> iter=2 : Found first='Ġw' in word[i:]=('a', 'a', 'a', 'Ġw', 'or', 'd'). Adding new token ('a', 'a', 'a')\n",
      "\t -> iter=3 : first='Ġw' not found in word[i:]=('d',)\n",
      "iter=3 : Updated word=('a', 'a', 'a', 'Ġwor', 'd'), pairs={('a', 'a'), ('Ġwor', 'd'), ('a', 'Ġwor')}\n",
      "iter=3 : first='Ġwor' second='d'\n",
      "\t -> iter=3 : Found first='Ġwor' in word[i:]=('a', 'a', 'a', 'Ġwor', 'd'). Adding new token ('a', 'a', 'a')\n",
      "iter=4 : Updated word=('a', 'a', 'a', 'Ġword'), pairs={('a', 'a'), ('a', 'Ġword')}\n",
      "iter=4 : first='a' second='a'\n",
      "\t -> iter=4 : Found first='a' in word[i:]=('a', 'a', 'a', 'Ġword'). Adding new token ()\n",
      "\t -> iter=5 : Found first='a' in word[i:]=('a', 'Ġword'). Adding new token ()\n",
      "\t -> iter=6 : first='a' not found in word[i:]=('Ġword',)\n",
      "iter=6 : Updated word=('aa', 'a', 'Ġword'), pairs={('aa', 'a'), ('a', 'Ġword')}\n",
      "iter=6 : first='aa' second='a'\n",
      "\t -> iter=6 : Found first='aa' in word[i:]=('aa', 'a', 'Ġword'). Adding new token ()\n",
      "\t -> iter=7 : first='aa' not found in word[i:]=('Ġword',)\n",
      "iter=7 : Updated word=('aaa', 'Ġword'), pairs={('aaa', 'Ġword')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'aaa Ġword'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_into_tokens(\"aaa word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's almost exactly the same steps, except that the higest priority tokens seem to keep falling towards the end of the tuple `word`, and thus the tokens at the end keep getting merged (Notice how the first three tokens `('a','a','a')` keep getting skipped in the first few iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MySlowTokenizer\n",
    "\n",
    "Let's now move on to the slow tokenizer's implementation. Let's see what you can _do_ with MySlowTokenizer first. Features implemented:\n",
    "- `tokenizer(text)` : Tokenizes a piece of text and returns a list of token ids. Equivalent to `tokenizer.encode(text)`\n",
    "- `tokenizer.decode(token_ids)`: Decodes a list of token ids and returns a stitched up string.\n",
    "- `tokenizer.add_tokens(my_new_tokens)`: Add new tokens to the tokenizer's vocabulary. If a token is already present, this errors out. \n",
    "- `tokenizer.convert_token_to_id(token)` : Self-evident\n",
    "- `tokenizer.convert_id_to_token(token_id)` : Self-evident\n",
    "- `tokenizer.pre_tokenize(text)`: Pretokenizes a string by splitting at whitespaces, contractions (Ex: `don't`), etc. This isn't a method in HuggingFace, but I find this convenient. \n",
    "- `tokenizer.to_dict()` : Export tokenizer state into a dictionary. (Sadly, a `from_dict` hasn't been implemented, as I think this might be overkill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySlowTokenizer(vocab_size=50257, unk_token=<|endoftext|>, added_tokens={'<|endoftext|>'}, bpe=BPE(vocab_size=50257))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minimal_hf_tok import MySlowTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=False)\n",
    "my_tokenizer = MySlowTokenizer(\"vocab.json\")\n",
    "my_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added <|myspecialtoken|> to the vocabulary.\n",
      "Input text: This isn't<|myspecialtoken|> that   simple\n",
      "My tokenizer encoding: [1212, 2125, 470, 50257, 326, 220, 220, 2829]\n",
      "GPT2 tokenizer encoding: [1212, 2125, 470, 50257, 326, 220, 220, 2829]\n",
      "My tokenizer decoding: This isn't<|myspecialtoken|> that   simple\n",
      "GPT2 tokenizer decoding: This isn't <|myspecialtoken|>  that   simple\n"
     ]
    }
   ],
   "source": [
    "input_text = \"This isn't<|myspecialtoken|> that   simple\"\n",
    "new_token = \"<|myspecialtoken|>\"\n",
    "my_tokenizer.add_tokens(new_token)\n",
    "gpt2_tokenizer.add_tokens(new_token)\n",
    "my_enc = my_tokenizer.encode(input_text)\n",
    "gpt2_enc = gpt2_tokenizer.encode(input_text)\n",
    "\n",
    "print(\"Input text:\", input_text)\n",
    "print(\"My tokenizer encoding:\", my_enc)\n",
    "print(\"GPT2 tokenizer encoding:\", gpt2_enc)\n",
    "\n",
    "print(\"My tokenizer decoding:\", my_tokenizer.decode(my_enc))\n",
    "print(\"GPT2 tokenizer decoding:\", gpt2_tokenizer.decode(gpt2_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tokenizer gives the same result as HuggingFace's tokenizer, and supports adding a new token!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pouring over the implementation\n",
    "I'm copying over the code from `minimal_hf_tok.py` here, because, well, this wouldn't be much of a walkthrough otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, Union, List, Any\n",
    "import regex as re # regex is cooler than re\n",
    "from bpe import BPE\n",
    "from minimal_hf_tok import EOS_TOKEN, MyTrie\n",
    "\n",
    "class MySlowTokenizer:\n",
    "    \"\"\"\n",
    "    A minimal implementation of HF's slow tokenizer, based on GPT2's tokenizer\n",
    "    References:\n",
    "    https://github.com/huggingface/transformers/blob/8aca43bdb3cb9a5020f6d57589d85679dc873b1c/src/transformers/models/gpt2/tokenization_gpt2.py\n",
    "    \"\"\"\n",
    "    def __init__(self, init_vocab_file: str = None):\n",
    "        self.added_tokens_trie = MyTrie() # trie for added tokens only\n",
    "        self.bpe = BPE(init_vocab_file)\n",
    "        self.vocab = self.bpe.token_to_id # nice to have vocab accessible here\n",
    "        self.byte_encoder = self.bpe.byte_encoder\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "        self.unk_token = EOS_TOKEN\n",
    "\n",
    "        # Regex for pre-tokenization - breaking up a piece of text into words by splitting at whitespaces, contractions, etc. Borrowed from GPT-2\n",
    "        self.pattern_for_splitting = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "        self._load_added_tokens()\n",
    "    \n",
    "    def _load_added_tokens(self):\n",
    "        # loads added tokens from json and adds them to the trie\n",
    "        # Hard coded for GPT2 demonstration\n",
    "        self.added_tokens_trie.add(EOS_TOKEN)\n",
    "    \n",
    "    def __call__(self, *args: Any, **kwargs: Any) -> Any:\n",
    "        self.encode(*args, **kwargs)\n",
    "    \n",
    "    def encode(self, text: str, **kwargs: Any) -> Any:\n",
    "        text, kwargs = self.prepare_for_tokenization(text, **kwargs)\n",
    "\n",
    "        # 1. Split text into chunks at the boundaries of added_tokens. Can be thought of as a pre-tokenization step.\n",
    "        # \"This isn't<|endoftext|> what you think\" -> [\"This isn't\", \"<|endoftext|>\", \" what you think\"] \n",
    "        chunks = self.added_tokens_trie.split(text)\n",
    "        bpe_tokens = []\n",
    "        for chunk in chunks:\n",
    "            if chunk in self.added_tokens_trie._tokens:\n",
    "                # if chunk is an added token, directly add it to bpe_tokens\n",
    "                bpe_tokens.append(chunk)\n",
    "            else:\n",
    "                # 2. Tokenize each chunk\n",
    "                tokens = self._tokenize(chunk)\n",
    "                bpe_tokens.extend(tokens)\n",
    "        # 3. Convert tokens to ids\n",
    "        bpe_tokens = [self.convert_token_to_id(token) for token in bpe_tokens]\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, ids: List[int], **kwargs: Any) -> str:\n",
    "        # 1. Convert ids to tokens\n",
    "        tokens = [self.convert_id_to_token(id_) for id_ in ids] \n",
    "        # 2. Join tokens\n",
    "        text = \"\".join(tokens)\n",
    "        # 3. Replace unicode symbols with normal characters\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\")\n",
    "        return text\n",
    "    \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        all_tokens = []\n",
    "        # Pre-tokenization: split text into words based on regex. \"This isn't\" -> [\"This\", \" isn\", \"'t\"]\n",
    "        words = self.pre_tokenize(text)\n",
    "        for word in words:\n",
    "            # Unicode string encoding. \" isn'\" -> bytes object -> \"Ġisn'\"\n",
    "            word = \"\".join([self.byte_encoder[b] for b in word.encode(\"utf-8\")]) \n",
    "            tokens = self.bpe(word, dont_byte_encode=True).split(\" \") # we already encoded the chunk to unicode strings\n",
    "            all_tokens.extend(tokens)\n",
    "        return all_tokens\n",
    "\n",
    "    def pre_tokenize(self, text: str) -> List[str]:\n",
    "        return self.pattern_for_splitting.findall(text)\n",
    "    \n",
    "    def prepare_for_tokenization(self, text: str, **kwargs: Any) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        In HF, this method performs any pre-processing needed before tokenization. Dummy function for now.\n",
    "        \"\"\"\n",
    "        # returns text and kwargs\n",
    "        return (text, kwargs)\n",
    "\n",
    "    def to_dict(self):\n",
    "        dict1 = {}\n",
    "        dict1[\"model\"][\"type\"] = \"BPE\"\n",
    "        dict1[\"model\"][\"vocab\"] = self.vocab\n",
    "        dict1[\"model\"][\"merges\"] = self.bpe.merges\n",
    "        dict1[\"added_tokens\"] = list(self.added_tokens_trie._tokens)\n",
    "        dict1[\"special_tokens_map\"] = {\"unk_token\": self.unk_token}\n",
    "        return dict1\n",
    "    \n",
    "    def add_tokens(self, new_tokens: Union[str, List[str]]):\n",
    "        \"\"\"\n",
    "        Adds new tokens to the tokenizer.\n",
    "        \"\"\"\n",
    "        if isinstance(new_tokens, str):\n",
    "            new_tokens = [new_tokens]\n",
    "        for token in new_tokens:\n",
    "            self.bpe.add_token(token) # add to vocab first\n",
    "            self.added_tokens_trie.add(token)\n",
    "            print(f\"Added {token} to the vocabulary.\")\n",
    "    \n",
    "    def convert_token_to_id(self, token: str) -> int:\n",
    "        \"\"\"\n",
    "        Converts a token to its id. Returns unk token id if token is not in vocab.\n",
    "        Fancy word: Numericalization\n",
    "        \"\"\"\n",
    "        return self.vocab.get(token, self.vocab[self.unk_token])\n",
    "\n",
    "    def convert_id_to_token(self, index: int) -> str:\n",
    "        \"\"\"\n",
    "        Converts an id to its token. Returns unk token if id is not in vocab.\n",
    "        \"\"\"\n",
    "        return self.bpe.id_to_token.get(index, self.unk_token)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        string = \"MySlowTokenizer(\"\n",
    "        string += f\"vocab_size={self.nvocab}, unk_token={self.unk_token}, added_tokens={str(self.added_tokens_trie._tokens)}, \"\n",
    "        string += f\"bpe={str(self.bpe)})\"\n",
    "        return string\n",
    "    \n",
    "    # make vocab_size a property because it should change with added tokens\n",
    "    @property\n",
    "    def nvocab(self) -> int:\n",
    "        return len(self.vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `__init__`\n",
    "\n",
    "1. Initializes the tokenizer with a vocabulary file if provided.\n",
    "2. Sets up the `MyTrie` structure for added tokens.\n",
    "3. Prepares the byte pair encoding (BPE) mechanism.\n",
    "4. Creates a byte encoder and decoder for character-level representation.\n",
    "5. Sets a pattern for pre-tokenization, which splits text into chunks based on specified rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `encode`\n",
    "\n",
    "Let's break down how to implement the `encode` method. When you do `tok.encode(text)`, there are three operations:\n",
    "1. Normalize and pre-tokenize input text. With GPT2, the pre-tokenization involves breaking up the text on whitespace, contractions, punctuations, etc.\n",
    "2. Tokenize input string/strings to get a list of tokens for each word/chunk. This is handled by the `._tokenize()` method.\n",
    "3. Convert tokens to token ids using `.convert_tokens_to_ids()` method.\n",
    "\n",
    "\n",
    "Important detail on `added_tokens`: These are really tokens added to the BPE vocabulary after the model was trained -  can you really just let the BPE model tokenize your string directly? Think about this:\n",
    "1. You added a new token `<|extraspecialtoken|>` to the vocabulary and gavae it a new ID.\n",
    "2. You tokenize a string `Hello there <|extraspecialtoken|>`. \n",
    "What happens if you directly use the BPE model to tokenize this string? First, BPE will split this up into symbols, and then it will iteratively merge neighbouring pairs of tokens/characters until you can't merge anymore. So, we need to handle this in the pre-tokenization step - Along with splitting on whitespace, punctuations, etc, we will also split at the boundaries of `added_tokens`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `decode`\n",
    "When you run `tok.decode(token_ids)`, there are three operations:\n",
    "1. Convert ids to tokens using the `id_to_token` mapping from `tok.bpe`. \n",
    "2. Join all the tokens\n",
    "3. Replace unicode symbols with normal characters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

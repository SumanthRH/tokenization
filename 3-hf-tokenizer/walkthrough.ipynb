{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple walkthrough for a minimal HuggingFace Tokenizer implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the jupyter notebook fans out there, this is a scrappy walkthrough for all the different parts in the implementation for the `BPE` and the `MySlowTokenizer` classes implemented here. If you haven't gone through the chapter README yet, please do! The gist is that HuggingFace's [PreTokenizer class](https://huggingface.co/docs/transformers/main_classes/tokenizer) implements a \"slow\" tokenizer in python. `PreTokenizer` inherits from `PreTokenizerBase`, and a `SpecialTokensMixin` class. Thus, if you're trying to understand the slow tokenizer for GPT-2, you have a whole class genealogy to figure out what is implemented where:\n",
    "```\n",
    "PreTokenizerBase       SpecialTokensMixin\n",
    "        \\                   /\n",
    "         \\                 /\n",
    "          PreTrainedTokenizer\n",
    "                  |\n",
    "                  |\n",
    "            GPT2Tokenizer\n",
    "```\n",
    "This is not pretty to navigate. So, I've tried to make things simple with just 2 classes:\n",
    "\n",
    "1. `BPE` : Meant to replicate what the BPE algorithm does in GPT-2's tokenizer. This is not a complete tokenizer in itself, it's got some weird kinks (with unicode symbols, etc) that we'll see soon.\n",
    "2. `MySlowTokenizer` : This is meant to a minimal implementation that can match all the basic features for HuggingFace's GPT2 tokenizer (the slow version).\n",
    "\n",
    "We're going to completely ignore the special tokens handling for now, as this is related to postprocessing that is in fact easier to understand later. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE\n",
    "Let's see our implementation in action first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sumanthrh/anaconda3/envs/huggingface/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bpe import BPE \n",
    "bpe = BPE(\"../2-bpe/vocab.json\") # make sure you're in the current directory of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `__call__` method\n",
    "Let's see what the `__call__` method does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bpe.py:56: UserWarning: Word contains whitespaces. Encoding to unicode strings...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ġword aaa'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe(\" wordaaa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output for \" wordaaa\" from the BPE tokenizer is the strange string \"Ġword aaa\". Well, here's what it does:\n",
    "1. Converts input string into bytes and encodes each byte with a unicode symbol. Specifically, a space \" \" becomes \"Ġ\". This is how GPT-2 does it. \n",
    "2. Tokenizes the string into characters \n",
    "3. Applies the BPE merge algorithm to iteratively combine intermediate tokens until you can't reduce it further.\n",
    "4. Join the list of tokens with a whitespace and return the resultant string\n",
    "\n",
    "Before we dive into the implementation for the merge algorithm, let's see what the list of tokens are when you use GPT2's tokenizer from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF's tokens:  [' word', 'aaa']\n",
      "My tokens:  ['Ġword', 'aaa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bpe.py:56: UserWarning: Word contains whitespaces. Encoding to unicode strings...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=False)\n",
    "text = \" wordaaa\"\n",
    "hf_tokens = gpt2_tokenizer.batch_decode(gpt2_tokenizer.encode(text))\n",
    "my_tokens = bpe(text).split(\" \")\n",
    "print(\"HF's tokens: \", hf_tokens)\n",
    "print(\"My tokens: \", my_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two outputs should infact be exactly the same, except for special characters (whitespace, etc) which we'll not get into later. (these two classes aren't exactly comparable as I mentioned, but it's good to see that this work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "Here's the complete merge algorithm with bpe, showing as a standalone function (with unicode symbols decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpe import get_pairs\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "byte_encoder = bytes_to_unicode()\n",
    "byte_decoder = {v: k for k, v in byte_encoder.items()}\n",
    "bpe_ranks = bpe.bpe_ranks\n",
    "\n",
    "def split_into_tokens(word: str):\n",
    "    word = \"\".join([byte_encoder[b] for b in word.encode(\"utf-8\")])\n",
    "    pairs = get_pairs(word) # \"obobc\" -> set([(\"o\", \"b\"), (\"b\", \"o\"), (\"b\", \"c\")])\n",
    "    iter = 0\n",
    "    while True:\n",
    "        # get pair of chars/tokens with lowest rank and merge\n",
    "        bigram = min(pairs, key=lambda pair: bpe_ranks.get(pair, float(\"inf\")))\n",
    "        if bigram not in bpe_ranks:\n",
    "            break # no more mergeable pairs\n",
    "        first, second = bigram\n",
    "        print(f\"{iter=} : {first=} {second=}\")\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i) # find index of occurence of `first` token in word[i:]\n",
    "            except ValueError:\n",
    "                print(f\"\\t -> {iter=} : {first=} not found in {word[i:]=}\")\n",
    "                new_word.extend(word[i:]) \n",
    "                break\n",
    "            else:\n",
    "                print(f\"\\t -> {iter=} : Found {first=} in {word[i:]=}. Skipping previous tokens {word[i:j]}\")\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "\n",
    "            if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
    "                print(f\"\\t -> {iter=} : Merging {first=} and {second=}\")\n",
    "                new_word.append(first + second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "            iter += 1\n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        if len(word) == 1: # merged into a single token\n",
    "            break\n",
    "        else:\n",
    "            pairs = get_pairs(word)\n",
    "        print(f\"{iter=} : Updated {word=}, {pairs=}\")\n",
    "    word = \" \".join(word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=0 : first='Ġ' second='w'\n",
      "\t -> iter=0 : Found first='Ġ' in word[i:]='Ġwordaaa'. Skipping previous tokens \n",
      "\t -> iter=0 : Merging first='Ġ' and second='w'\n",
      "\t -> iter=1 : first='Ġ' not found in word[i:]='ordaaa'\n",
      "iter=1 : Updated word=('Ġw', 'o', 'r', 'd', 'a', 'a', 'a'), pairs={('d', 'a'), ('a', 'a'), ('r', 'd'), ('Ġw', 'o'), ('o', 'r')}\n",
      "iter=1 : first='o' second='r'\n",
      "\t -> iter=1 : Found first='o' in word[i:]=('Ġw', 'o', 'r', 'd', 'a', 'a', 'a'). Skipping previous tokens ('Ġw',)\n",
      "\t -> iter=1 : Merging first='o' and second='r'\n",
      "\t -> iter=2 : first='o' not found in word[i:]=('d', 'a', 'a', 'a')\n",
      "iter=2 : Updated word=('Ġw', 'or', 'd', 'a', 'a', 'a'), pairs={('Ġw', 'or'), ('or', 'd'), ('d', 'a'), ('a', 'a')}\n",
      "iter=2 : first='Ġw' second='or'\n",
      "\t -> iter=2 : Found first='Ġw' in word[i:]=('Ġw', 'or', 'd', 'a', 'a', 'a'). Skipping previous tokens ()\n",
      "\t -> iter=2 : Merging first='Ġw' and second='or'\n",
      "\t -> iter=3 : first='Ġw' not found in word[i:]=('d', 'a', 'a', 'a')\n",
      "iter=3 : Updated word=('Ġwor', 'd', 'a', 'a', 'a'), pairs={('d', 'a'), ('Ġwor', 'd'), ('a', 'a')}\n",
      "iter=3 : first='Ġwor' second='d'\n",
      "\t -> iter=3 : Found first='Ġwor' in word[i:]=('Ġwor', 'd', 'a', 'a', 'a'). Skipping previous tokens ()\n",
      "\t -> iter=3 : Merging first='Ġwor' and second='d'\n",
      "\t -> iter=4 : first='Ġwor' not found in word[i:]=('a', 'a', 'a')\n",
      "iter=4 : Updated word=('Ġword', 'a', 'a', 'a'), pairs={('Ġword', 'a'), ('a', 'a')}\n",
      "iter=4 : first='a' second='a'\n",
      "\t -> iter=4 : Found first='a' in word[i:]=('Ġword', 'a', 'a', 'a'). Skipping previous tokens ('Ġword',)\n",
      "\t -> iter=4 : Merging first='a' and second='a'\n",
      "\t -> iter=5 : Found first='a' in word[i:]=('a',). Skipping previous tokens ()\n",
      "iter=6 : Updated word=('Ġword', 'aa', 'a'), pairs={('aa', 'a'), ('Ġword', 'aa')}\n",
      "iter=6 : first='aa' second='a'\n",
      "\t -> iter=6 : Found first='aa' in word[i:]=('Ġword', 'aa', 'a'). Skipping previous tokens ('Ġword',)\n",
      "\t -> iter=6 : Merging first='aa' and second='a'\n",
      "iter=7 : Updated word=('Ġword', 'aaa'), pairs={('Ġword', 'aaa')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ġword aaa'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_into_tokens(\" wordaaa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how with each iteration, BPE merges the pair of tokens with the lowest rank / highest priority. It has to merge all occurences of the pair, and thus you have two while loops. The brief summary of what happened:\n",
    "1. Obtain all unique bigrams (pairs of adjacent symbols) from the word. (before the while loop)\n",
    "2. State: `word` is initially a string, but it is converted into a tuple in later iterations. `word` represents the current segmentation of the original word, represented as a tuple of tokens.\n",
    "3. The algorithm then enters a loop where it looks for the lowest-ranked bigram (the first merge that BPE learnt while training). This represents the most frequent pair to be merged, or rather, the best compression step that BPE knows for this word.\n",
    "4. If the bigram is not in `bpe_ranks`, it means that no more merges are possible, and the process terminates.\n",
    "    - Why? Observe how the minimum has been calculated. If the bigram with minimum rank is not in `bpe_ranks`, then it has a rank of `inf`, which means none of the other bigrams are in `bpe_ranks` / merge list.\n",
    "5. Otherwise, the function reconstructs the word by merging instances of the bigram.\n",
    "    - This is the inner while loop. We iteratively find the first occurence of `first` in `word`, record the index, and break up the word into two at this index. Keep proceeding till the end of `word`.\n",
    "6. This process continues until the word cannot be further merged, at which point the final tokenized word is returned by joining all the tokens with a whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we flipped the order of the tokens? How do the merges look then? Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=0 : first='Ġ' second='w'\n",
      "\t -> iter=0 : Found first='Ġ' in word[i:]='aaaĠword'. Adding new token aaa\n",
      "\t -> iter=1 : first='Ġ' not found in word[i:]='ord'\n",
      "iter=1 : Updated word=('a', 'a', 'a', 'Ġw', 'o', 'r', 'd'), pairs={('a', 'a'), ('r', 'd'), ('Ġw', 'o'), ('o', 'r'), ('a', 'Ġw')}\n",
      "iter=1 : first='o' second='r'\n",
      "\t -> iter=1 : Found first='o' in word[i:]=('a', 'a', 'a', 'Ġw', 'o', 'r', 'd'). Adding new token ('a', 'a', 'a', 'Ġw')\n",
      "\t -> iter=2 : first='o' not found in word[i:]=('d',)\n",
      "iter=2 : Updated word=('a', 'a', 'a', 'Ġw', 'or', 'd'), pairs={('Ġw', 'or'), ('or', 'd'), ('a', 'a'), ('a', 'Ġw')}\n",
      "iter=2 : first='Ġw' second='or'\n",
      "\t -> iter=2 : Found first='Ġw' in word[i:]=('a', 'a', 'a', 'Ġw', 'or', 'd'). Adding new token ('a', 'a', 'a')\n",
      "\t -> iter=3 : first='Ġw' not found in word[i:]=('d',)\n",
      "iter=3 : Updated word=('a', 'a', 'a', 'Ġwor', 'd'), pairs={('a', 'a'), ('Ġwor', 'd'), ('a', 'Ġwor')}\n",
      "iter=3 : first='Ġwor' second='d'\n",
      "\t -> iter=3 : Found first='Ġwor' in word[i:]=('a', 'a', 'a', 'Ġwor', 'd'). Adding new token ('a', 'a', 'a')\n",
      "iter=4 : Updated word=('a', 'a', 'a', 'Ġword'), pairs={('a', 'a'), ('a', 'Ġword')}\n",
      "iter=4 : first='a' second='a'\n",
      "\t -> iter=4 : Found first='a' in word[i:]=('a', 'a', 'a', 'Ġword'). Adding new token ()\n",
      "\t -> iter=5 : Found first='a' in word[i:]=('a', 'Ġword'). Adding new token ()\n",
      "\t -> iter=6 : first='a' not found in word[i:]=('Ġword',)\n",
      "iter=6 : Updated word=('aa', 'a', 'Ġword'), pairs={('aa', 'a'), ('a', 'Ġword')}\n",
      "iter=6 : first='aa' second='a'\n",
      "\t -> iter=6 : Found first='aa' in word[i:]=('aa', 'a', 'Ġword'). Adding new token ()\n",
      "\t -> iter=7 : first='aa' not found in word[i:]=('Ġword',)\n",
      "iter=7 : Updated word=('aaa', 'Ġword'), pairs={('aaa', 'Ġword')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'aaa Ġword'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_into_tokens(\"aaa word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's almost exactly the same steps, except that the higest priority tokens seem to keep falling towards the end of the tuple `word`, and thus the tokens at the end keep getting merged (Notice how the first three tokens `('a','a','a')` keep getting skipped in the first few iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MySlowTokenizer\n",
    "\n",
    "Let's now move on to the slow tokenizer's implementation. Let's see what you can _do_ with MySlowTokenizer first. Features implemented:\n",
    "- `tokenizer(text)` : Tokenizes a piece of text and returns a list of token ids. Equivalent to `tokenizer.encode(text)`\n",
    "- `tokenizer.decode(token_ids)`: Decodes a list of token ids and returns a stitched up string.\n",
    "- `tokenizer.add_tokens(my_new_tokens)`: Add new tokens to the tokenizer's vocabulary. If a token is already present, this errors out. \n",
    "- `tokenizer.convert_token_to_id(token)` : Self-evident\n",
    "- `tokenizer.convert_id_to_token(token_id)` : Self-evident\n",
    "- `tokenizer.pre_tokenize(text)`: Pretokenizes a string by splitting at whitespaces, contractions (Ex: `don't`), etc. This isn't a method in HuggingFace, but I find this convenient. \n",
    "- `tokenizer.to_dict()` : Export tokenizer state into a dictionary. (Sadly, a `from_dict` hasn't been implemented, as I think this might be overkill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySlowTokenizer(vocab_size=50257, unk_token=<|endoftext|>, added_tokens={'<|endoftext|>'}, bpe=BPE(vocab_size=50257))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minimal_hf_tok import MySlowTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=False)\n",
    "my_tokenizer = MySlowTokenizer(\"../2-bpe/vocab.json\")\n",
    "my_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added <|myspecialtoken|> to the vocabulary.\n",
      "Input text: This isn't<|myspecialtoken|> that   simple\n",
      "My tokenizer encoding: [1212, 2125, 470, 50257, 326, 220, 220, 2829]\n",
      "GPT2 tokenizer encoding: [1212, 2125, 470, 50257, 326, 220, 220, 2829]\n",
      "My tokenizer decoding: This isn't<|myspecialtoken|> that   simple\n",
      "GPT2 tokenizer decoding: This isn't <|myspecialtoken|>  that   simple\n"
     ]
    }
   ],
   "source": [
    "input_text = \"This isn't<|myspecialtoken|> that   simple\"\n",
    "new_token = \"<|myspecialtoken|>\"\n",
    "my_tokenizer.add_tokens(new_token)\n",
    "gpt2_tokenizer.add_tokens(new_token)\n",
    "my_enc = my_tokenizer.encode(input_text)\n",
    "gpt2_enc = gpt2_tokenizer.encode(input_text)\n",
    "\n",
    "print(\"Input text:\", input_text)\n",
    "print(\"My tokenizer encoding:\", my_enc)\n",
    "print(\"GPT2 tokenizer encoding:\", gpt2_enc)\n",
    "\n",
    "print(\"My tokenizer decoding:\", my_tokenizer.decode(my_enc))\n",
    "print(\"GPT2 tokenizer decoding:\", gpt2_tokenizer.decode(gpt2_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tokenizer gives the same result as HuggingFace's tokenizer, and supports adding a new token!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `encode`\n",
    "\n",
    "Let's break down how to implement the `encode` method. There are three operations:\n",
    "1. Normalize and pre-tokenize input text. With GPT2, the pre-tokenization involves breaking up the text on whitespace, contractions, punctuations, etc.\n",
    "2. Tokenize input string/strings to get a list of tokens for each word/chunk. This is handled by the `.tokenize()` method.\n",
    "3. Convert tokens to token ids using `.convert_tokens_to_ids()` method.\n",
    "\n",
    "\n",
    "Important detail on `added_tokens`: These are really tokens added to the BPE vocabulary after the model was trained -  can you really just let the BPE model tokenize your string directly? Think about this:\n",
    "1. You added a new token `<|extraspecialtoken|>` to the vocabulary and gavae it a new ID.\n",
    "2. You tokenize a string `Hello there <|extraspecialtoken|>`. \n",
    "What happens if you directly use the BPE model to tokenize this string? First, BPE will split this up into symbols, and then it will iteratively merge neighbouring pairs of tokens/characters until you can't merge anymore. So, we need to handle this in the pre-tokenization step - Along with splitting on whitespace, punctuations, etc, we will also split at the boundaries of `added_tokens`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

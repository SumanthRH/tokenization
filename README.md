# Tokenization

Tokenization is an oft-neglected part of natural language processing. With the recent blow-up of interest in language models, it might be good to step back and really get into the guts of what tokenization is, the different tokenization algorithms out there, the internals of HuggingFace tokenizers, the impact of tokenization on sequence lengths, multilingual training etc. 

